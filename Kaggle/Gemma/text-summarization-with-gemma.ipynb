{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":6140902,"sourceType":"datasetVersion","datasetId":3475059},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U -q transformers langchain peft bitsandbytes trl datasets notebook accelerate evaluate","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.core.display import HTML\ntable_css = \"\"\"\n    table {\n        align: left; display: block\n    }\n\"\"\"\nHTML('<style>{}</style>'.format(table_css))","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-04T00:04:08.765554Z","iopub.execute_input":"2024-03-04T00:04:08.765916Z","iopub.status.idle":"2024-03-04T00:04:08.780741Z","shell.execute_reply.started":"2024-03-04T00:04:08.765880Z","shell.execute_reply":"2024-03-04T00:04:08.779825Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table {\n        align: left; display: block\n    }\n</style>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Text summarization with Gemma and LangChain\n\n# Table of Contents\n1. [Introduction](#Introduction)<br>\n    1.1. [Aim of the project](#Aim-of-the-project)<br>\n2. [Setup and important aspects](#Setup-and-important-aspects)<br>\n    2.1. [Chat template](#Chat-template)<br>\n    2.2. [Prompt engineering](#Prompt-engineering)<br>\n    2.3. [Pipeline parameters](#Pipeline-parameters)<br>\n3. [Text summarization: Methods and strategies](#Text-summarization:-Methods-and-strategies)<br>\n    3.1. [Stuffing](#Stuffing)<br>\n    3.2. [MapReduce](#MapReduce)<br>\n    3.3. [Refine](#Refine)<br>\n    3.4. [Document splitting strategies](#Document-splitting-strategies)<br>\n4. [Experiments](#Experiments)<br>\n5. [Fine-tuning Gemma with LoRa](#Fine-tuning-Gemma-with-LoRa)\n6. [Conclusions and next steps](#Conclusions-and-next-steps)\n\n# Introduction\n\nOver the years, the amount of data produced, copied and consumed in the world has grown exponentially, soaring from 2 Zettabytes in 2010 to projected estimates of [181 Zettabytes in 2025](https://www.statista.com/statistics/871513/worldwide-data-created/). <br>\nTo put these numbers into context, envision each byte as a grain of rice; one zettabyte (10^21 bytes) would be somewhat equivalent to filling [the Pacific Ocean with rice](https://www.oldcolony.us/wp-content/uploads/2014/11/whatisbigdata-DKB-v2.pdf).\n\nIn this context, one of the most common and useful tasks in the NLP field is **text summarization.** <br>\nSummarization is the process of extracting the **most important information from a text** and presenting it in a condensed form. Having quality condensed information can help individuals and organizations **reduce this information overload**, [optimizing processes while saving time and resources](https://nowigence.com/importance-benefits-of-auto-text-summarization/).\n\nThere are **mainly two** text summarization techniques: abstractive and extractive.\n\n- **Abstractive**: The text is summarized using the available context but using different words. This is a process that requires \"understanding\" the information contained in the text.\n- **Extractive**: The most relevant phrases or words are selected and extracted from the text, without rephrasing or generating new words.\n\nThe advent of LLM models (like Gemma) helped to **significantly improve** the [quality of the summaries produced](https://arxiv.org/pdf/2310.10449.pdf), so much that LLMs are now considered the [gold standard for text summarization](https://arxiv.org/pdf/2305.14239.pdf).\n\nHowever, there are **some considerations** to bear in mind when undertaking this task, in particular:\n\n1. **Quality**: although there are metrics like ROUGE to assess quality, a summary is often subjective and varies based on the target audience. Is the summary intended for a technical or non-technical audience? Should it provide detailed information or offer a high-level overview?\n2. **Hallucinations**: it is known that LLMs tend to have [hallucinations](https://arxiv.org/pdf/2401.11817.pdf), that is to generate plausible but incorrect information from a factual or logical point of view. This phenomenon can have significant impacts, especially when dealing with large documents where content is not known in advance, making it challenging to control.\n3. **Text length**: every LLM have a maximum context window and large documents can't fit entirely in one pass, thus different approaches are required.\n\n\n## Aim of the project\n\nIn this notebook, I will demonstrate the process of **text summarization using Gemma**, with a dedicated emphasis on **Kaggle writeups.** <br>\nThe key aspects I will discuss are:\n- Establishing a text summarization pipeline using Gemma and LangChain\n- Providing an overview of the crucial parameters and methods one should keep in mind while working with an LLM\n- Exploring summarization techniques, such as Stuffing, MapReduce and Refine\n- Fine-tuning Gemma using Parameter Efficient Fine-Tuning (PEFT)\n- Future considerations and next steps\n\nThis work aims to build a **comprehensive understanding of the task** and develop a pipeline that can serve as a **good starting point** for indkividuals interested in approaching summarization tasks using open-source models like Gemma on Kaggle.\n\n---\n\n# Setup and important aspects\n\nThis notebook will use the following building blocks:\n- **Gemma 2B**, in order to work even on commercial laptop with common GPUs\n- **HuggingFace**, which thanks to its abstraction levels allows you to work with LLM in a user-friendly way\n- **LangChain**, to build summarization pipelines even in the presence of large documents\n\nLet's start importing the model and pipeline that we will use with HuggingFace:","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, set_seed\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom accelerate.utils import release_memory\nimport torch\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nfrom datasets import Dataset\nfrom trl import SFTTrainer\nfrom peft import LoraConfig, PeftModel\nimport pandas as pd\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain.text_splitter import CharacterTextSplitter, HTMLHeaderTextSplitter\nfrom langchain.prompts import PromptTemplate\nfrom langchain.docstore.document import Document\nfrom langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\nimport evaluate\nimport transformers\nfrom langchain.llms.base import LLM\nfrom typing import Any\nimport warnings\nimport gc\nimport random\nimport numpy as np\n\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Read writeups dataset\nwriteups = pd.read_csv('/kaggle/input/kaggle-winning-solutions-methods/kaggle_winning_solutions_methods.csv')\nwriteups = writeups.drop_duplicates(subset=['link', 'writeup']).reset_index(drop=True)\n\n# Logging in HF\nhf_access_token = UserSecretsClient().get_secret(\"hf_token\")\nlogin(token = hf_access_token)\n\nmodel = \"/kaggle/input/gemma/transformers/2b-it/1\"\n\n# Load the HF pipeline using Gemma 2B from Kaggle\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    model_kwargs={\"torch_dtype\": torch.float16},\n    device='cuda',\n    max_new_tokens=512\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-04T00:04:08.814733Z","iopub.execute_input":"2024-03-04T00:04:08.815081Z","iopub.status.idle":"2024-03-04T00:05:02.188721Z","shell.execute_reply.started":"2024-03-04T00:04:08.815054Z","shell.execute_reply":"2024-03-04T00:05:02.187891Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-04 00:04:13.452683: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-04 00:04:13.452779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-04 00:04:13.600307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a71c1ed444874f33a116e60e3509b540"}},"metadata":{}}]},{"cell_type":"markdown","source":"Pipelines provide an efficient and user-friendly way to leverage models for inference. They consist of:\n- A tokenizer, which, if not explicitly specified, is automatically imported from the model configurations on HuggingFace\n- The model itself\n- Parameters for controlling and fine-tuning the output\n\nConsidering the code above, several crucial parameters have been configured:\n- `max_new_tokens` - controls the **maximum number of newly generated tokens**. If not specified, the default value may not be sufficient to generate enough text (therefore summaries).\n- `model_kwargs` - here we control the **precision** using `torch.float16`, with beneficial effects on [memory](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#:~:text=Mixed%20precision%20training%20achieves%20all,bit%20floating%20point%20everywhere%20else.).\n\nNow let's test our pipeline on a the first writeup from our dataset, taken from [here](https://www.kaggle.com/c/asl-signs/discussion/406306):","metadata":{}},{"cell_type":"code","source":"# Import the first writeup from the dataset and inspect the first 1000 chars\nwriteup = writeups.iloc[0, 9]\nprint('Number of characters:', len(writeup))\nwriteup[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:02.190234Z","iopub.execute_input":"2024-03-04T00:05:02.190524Z","iopub.status.idle":"2024-03-04T00:05:02.197689Z","shell.execute_reply.started":"2024-03-04T00:05:02.190499Z","shell.execute_reply":"2024-03-04T00:05:02.196849Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of characters: 9864\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'<h2>TLDR</h2>\\n<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.</p>\\n<p>We used only competition data.</p>\\n<h2>1. Data Preprocessing</h2>\\n<h3>1.1 CNN Preprocessing</h3>\\n<ul>\\n<li>We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points.</li>\\n<li>During training, we applied various augmentations.</li>\\n<li>We implemented standard normalization.</li>\\n<li>Instead of dropping NaN values, we filled them with zeros after normalization.</li>\\n<li>We interpolated the time axis to a siz'"},"metadata":{}}]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\\n\\n{}\".format(writeup)\n    }\n]\n\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(\n    prompt,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3,\n    add_special_tokens=True\n)\nprint(outputs[0][\"generated_text\"][len(prompt):])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:02.199217Z","iopub.execute_input":"2024-03-04T00:05:02.199553Z","iopub.status.idle":"2024-03-04T00:05:24.638656Z","shell.execute_reply.started":"2024-03-04T00:05:02.199518Z","shell.execute_reply":"2024-03-04T00:05:24.637667Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Sure, here's a summary of the text in a technical way:\n\n**1. Data Preprocessing**\n\n* Extract 80 points from the image, including lip and pose points.\n* Apply various augmentations and normalizations.\n* Fill NaN values with zeros and use nearest interpolation for the time axis.\n\n**2. Augmentation**\n\n* Use common and CNN specific augmentations.\n* Implement a mixup augmentation that only works with CNNs.\n\n**3. Training**\n\n* Train EfficientNet-B0 and BERT on a single fold with 0.1 warm-up.\n* Train a transformer model with a ranger optimizer and 4-layer transformer.\n* Tune hyperparameters with Optuna.\n\n**4. Submissions**\n\n* Aggregate models in a tf.Module.\n* Calculate ensemble weights for fold 0 and apply to the full dataset.\n\n**5. PS. Need BETTER TFlite DepthwiseConv2D**\n\n* Explore different ways to implement depthwise convolution in tflite.\n* Experiment with different FLOP configurations.\n\n**6. Conclusion**\n\n* EfficientNet-B0 achieved a leaderboard score of 0.8.\n* Transformers improved the score to 0.81.\n* Ensemble of models with different architectures achieved the highest score of 0.82.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The output looks great, and this was possible thanks to **key features that we will now explore.**\n\n## Chat template\nThe main use of LLMs is in a **chat style setup**. This means that instead of continuing a text string, the model receives messages in which **\"roles\" are present.** <br>\nJust as there are different tokenizers for different models, each model expects different chat templates.\n\n[Gemma's technical documentation](https://ai.google.dev/gemma/docs/formatting) outlines the necessity of employing specific tokens to indicate roles:\n- Token to indicate a user turn: `user`\n- Token to indicate a model turn: `model`\n- Token to indicate the beginning of dialogue turn: `<start_of_turn>`\n- Token to indicate the end of dialogue turn: `<end_of_turn>`\n\nIn the code above, we achieved that thanks to the presence of the following piece of code:\n\n```\nmessages = [\n    {\"role\": \"user\",\n     \"content\": \"....\"}\n     ]\n\nprompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n```\nTo better understand what is happening, let's try a test chat:","metadata":{}},{"cell_type":"code","source":"test_messages = [\n    {\"role\": \"user\",\n     \"content\": \"This is a test\"},\n    {\"role\": \"assistant\",\n     \"content\": \"Good for you!\"},\n    {\"role\": \"user\",\n     \"content\": \"Ah ah\"},\n]\n\ntest_prompt = pipe.tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\nprint(test_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:24.641473Z","iopub.execute_input":"2024-03-04T00:05:24.642421Z","iopub.status.idle":"2024-03-04T00:05:24.648259Z","shell.execute_reply.started":"2024-03-04T00:05:24.642390Z","shell.execute_reply":"2024-03-04T00:05:24.647365Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<start_of_turn>user\nThis is a test<end_of_turn>\n<start_of_turn>model\nGood for you!<end_of_turn>\n<start_of_turn>user\nAh ah<end_of_turn>\n<start_of_turn>model\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Given that LLMs generate text predicting the next token, [HuggingFace](https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts) provides the `apply_chat_template` function to **ensure the model generates text as a response to an input** rather than as a continuation of the user's prompt. This is achieved using the `add_generation_prompt` parameter, which adds the `<start_of_turn>` token, **reducing the possibility of generating text that continues the user's message.**\n\n## Prompt engineering\n\nObtaining quality output also depends on the prompt structure. <br>\nThe query used was:\n```\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary into chapters, be impersonal and use bullet points:\n\n[writeup]\n```\nDepending on the audience, this **prompt may vary** to allow us to generate summaries more **aligned with our target.** <br>\nFor example, let's assume we need to create a writeup summary to a less technical audience:","metadata":{}},{"cell_type":"code","source":"messages_eli5 = [\n    {\"role\": \"user\",\n     \"content\": \"Summarize the following text while avoiding difficult jargon using bullet points chapters. Explain it like I am a 5 years old:\\n\\n{}\".format(writeup)},\n]\n\nprompt_eli5 = pipe.tokenizer.apply_chat_template(messages_eli5, tokenize=False, add_generation_prompt=True)\noutputs_eli5 = pipe(\n    prompt_eli5,\n    add_special_tokens=True,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3\n)\nprint(outputs_eli5[0][\"generated_text\"][len(prompt_eli5):])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:24.649272Z","iopub.execute_input":"2024-03-04T00:05:24.649551Z","iopub.status.idle":"2024-03-04T00:05:40.205346Z","shell.execute_reply.started":"2024-03-04T00:05:24.649517Z","shell.execute_reply":"2024-03-04T00:05:40.204410Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Sure, here's a summary of the text in a way that a 5-year-old might understand:\n\n<h2>Introduction</h2>\nWe used an approach similar to audio spectrogram classification using the EfficientNet-B0 model.\n\n<h3>Data Preprocessing</h3>\nWe extracted 80 points from the image, including lip and body points. We also used augmentation to make the data more diverse.\n\n<h2>Training</h2>\nWe trained the model on one fold with a random split. We used a one-cycle scheduler and weighted cross-entropy loss.\n\n<h3>Hyperparameter Tuning</h2>\nWe tuned the learning rate, dropout rate, and other parameters to find the best settings for the model.\n\n<h2>Results</h2>\nWe got a leaderboard score of 0.81, which is pretty good.\n\n<h2>Conclusion</h2>\nWe used a combination of EfficientNet-B0, BERT, and DeBERTa to achieve this result.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The summary is easier to understand, simply by adding `Explain it like I am a 5 years old`.\n\nIt's worth mentioning that there are **other techniques** that can help achieve better results, one of this is called called [Few-Shot Prompting](https://www.promptingguide.ai/techniques/fewshot). <br>\nFew shot prompring involves utilizing **examples as conditioning for subsequent examples, guiding the model in generating the desired responses.**\n\nLet's demonstrate few shot conditioning using the pipeline we developed so far:","metadata":{}},{"cell_type":"code","source":"messages_few_shot = [\n    {\"role\": \"user\",\n     \"content\": \"This film was great, rich of details and with great actors.\"},\n    {\"role\": \"assistant\",\n     \"content\": \"SENTIMENT: Positive.\\nSUBJECT: Film\"},\n    {\"role\": \"user\",\n     \"content\": \"This park is dirty.\"},\n    {\"role\": \"assistant\",\n     \"content\": \"SENTIMENT: Negative.\\nSUBJECT: Park\"},\n    {\"role\": \"user\",\n     \"content\": \"This notebook is fantastic. I'm learning a lot\"},\n]\n\nprompt_few_shot = pipe.tokenizer.apply_chat_template(messages_few_shot, tokenize=False, add_generation_prompt=True)\noutputs_few_shot = pipe(\n    prompt_few_shot,\n    add_special_tokens=True,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3\n)\nprint(outputs_few_shot[0][\"generated_text\"][len(prompt_few_shot):])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:40.206423Z","iopub.execute_input":"2024-03-04T00:05:40.206702Z","iopub.status.idle":"2024-03-04T00:05:40.927411Z","shell.execute_reply.started":"2024-03-04T00:05:40.206677Z","shell.execute_reply":"2024-03-04T00:05:40.926268Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"SENTIMENT: Positive.\nSUBJECT: Notebook<end_of_turn>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The model learned how to perform the task **with just few examples using the `apply_chat_template`.** Without any request, it correctly understood that we wanted two lines, one with `SENTIMENT` and one with the `SUBJECT` of the sentence.\n\nWhile applying Few-Shot Learning could be viable for our scenario, **it would require examples of real summarization of Kaggle writeups passed as examples**, potentially resulting in **overly large prompte**. Considering that the length of a text can be problematic (more on that later), **we will stick to a simpler approach** by working on out prompt engineering skills like we did before which already yielded good results.","metadata":{}},{"cell_type":"markdown","source":"## Pipeline parameters\n\nYou noticed that the pipeline uses few paramenters that are essential for controlling our output. <br>\nHere's the list along with some rationale:\n\n- `do_sample`: this parameter enables **decoding strategies** to select the next token from the probability distribution over the entire vocabulary. Together with `num_beams`, we can control [different strategies](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin). I opted for `True` and `num_beams=1` (default), which is the multinomial sampling. More of decoding strategies [here](https://deci.ai/blog/llm-evaluation-and-how-decoding-strategies-impact-instruction-following/) and [here](https://huggingface.co/docs/transformers/main/en/generation_strategies#decoding-strategies).\n- `temperature`: this parameter controls the **randomness**. The lower the temperature, the more deterministic the results are in the sense that the highest probable token is picked. I opted for a very low value because we need to encourage **more factual responses** and not creative ones.\n- `top_p`: according to the [documentation](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.top_p), this parameter controls the **sampling of tokens**. The higher the value, the higher the chances that the model will look to more possible words, including less likely ones. Again, I opted for a relatively low value **to maintain coherence** given the task of summarization.\n- `top_k`: in simple terms, together with `top_p`, [it controls the number of tokens](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.top_k) to keep for prediction. Once again, a low value will favour **less creative responses**, which is exactly what we are looking for in this notebook.\n\n<div class=\"alert alert-block alert-info\">\n<b>Key learnings </b><br> <br>\n    - Following a <b>chat template</b> is highly recommented in order to <b>mimick model's training process</b> (therefore, model's knowledge).<br>\n    - <b>Prompt engineering</b> is mandatory: a poor prompt will lead to poor results. Techniques such as <b>Few-Shot learning</b> can be useful tools in our arsenale. <br>\n    - Controlling the generation <b>parameters</b> is important and depends on the task, whether <b>we seek creativity or factual responses.</b>\n</div>\n\nNow it's time to talk about some **important aspects** of text summarization and **LangChain!**\n\n---\n\n# Text summarization: Methods and strategies\n\nThere are many techniques and ways to perform a text summarization, but I will focus only on **three strategies: Stuffing, MapReduce and Refine.** These ideas where based on this amazing blog post you can find [here](https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed).\nThe methods are simple and easy to understand, yet quite powerful.\n\n## Stuffing\n\n<img src=\"https://i.imgur.com/28BXaOG.png\" width=\"600\">\n\nStuffing is pretty straightforward: we **pass the entire data** to the LLM by stuffing it into the prompt as context. **This is exactly what we did so far using Gemma.**\n\n| Pros ✅| Cons ❌|\n|---|---|\n| Only a single call to the LLM | The data can surpass the model's context length, thus this method could not be feasible <br> for larger text files |\n| Comprehensive context, since the model have access to the <br> entire information | The quality might not be ideal for extensive  documents, <br> as some information might be skipped |\n\n\n## MapReduce\n\n<img src=\"https://i.imgur.com/6TJbU8V.png\" width=\"600\">\n\nMapReduce introduces a **multi-stage summarization**:\n- First we split the document into chunks\n- We perform text summarization for each chunk\n- One final call to the LLM is used to create a comprehensive final summary, using all the summaries as input\n\n| Pros ✅ | Cons ❌|\n|---|---|\n| The context limit is no longer a problem, since the document <br> is broken in manageable chunks | Multiple LLM calls are required, affecting processing time |\n| Each chunk can be process in parallel, thus speeding <br> the summarization task | Potential loss of information due to the fact that the LLM only sees each chunk <br> indipendently without a context |\n\n\n## Refine\n\n<img src=\"https://i.imgur.com/CBmwG10.png\" width=\"600\">\n\nThe last method is called Refine, and follows an **iterative approach**:\n- The document is split into chunks, just like the MapReduce\n- The first chunk is summarized\n- For each following chunk, the previous output (summary) is combined with the new information\n- The LLM is instructed to improve (refine!) the previous summary\n\n| Pros ✅| Cons ❌|\n|---|---|\n| It solves MapReduce's potential loss of information while retaining <br> the ability to process very large files| Multiple LLM calls are required affecting processing time |\n| It follows a sequentiality, thus potentially improve summary quality | LLM errors and hallucinations can propagade during each iteration, <br>thus affecting the final quality |\n\n\n## Document splitting strategies\n\nMapReduce and Refine depend heavily on document splitting. While the concept of splitting is easy to understand, **this step is often intricate.**\n\nFirst of all, when we deal with LLM we deal with **tokens**. Tokens are [pieces of words](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them): when we write a prompt, the input is transformed into tokens. One token doens't mean one work, but we can generally approximate **1 token ~ 4 characters in English ~ 3/4 of a word**. Simply put, 75 words ~ 100 tokens. <br>\nDepending on the model used, we can accomodate a certain amount of tokens **shared between prompt and model's generation**, thus forcing us to operate some **splitting if the context is too large**. [Gemma](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf) has a **maximum context length of 8192 tokens**, which roughly translates to more than **6100 words.**\n\nDocument splitting can follow different strategies. For example, it can be based on a **character** (`'\\n'` or `'\\n\\n'` which often delimit new sentences or paragraphs) or on the **document structure** (such as chapters or sections etc). <br>\nThe choice of the splitting strategy depends on the task and document we need to analyze. For our task, it is reasonable assume the following scenarios:\n- **Scenario A - Stuffing is feasible**: This is the best case scenario, we can simply pass the entire context to the model\n- **Scenario B - Stuffing is feasible but output is poor**: Given that it's unlikely that a writeup exceeds 6100 words, this scenario could arise if the quality of the Stuffing summarization is poor. Our model might skip some useful information if provided with the entire text, therefore a possible approach could be splitting writeups based on sections. The Kaggle template works well because is divided into clear sections, allowing for a smooth split while keeping semantic sentences in the same split.\n- **Scenario C - Output is poor and writeups don't follow a clear structure**: This is the most difficult, yet plausible, scenario, in which our model struggles with the winner's stream of consciousness and lack of a clear document structure. Moreover, if the writeup is lengthy, the situation could be particularly challenging. In such cases, a character splitting strategy could be ideal.\n\nLet's see it in action, testing a nicely formatted example writeup from the [documentation](https://www.kaggle.com/solution-write-up-documentation) and a messy one.","metadata":{}},{"cell_type":"code","source":"example_clean_writeup = \"\"\"\n# Context section\nThis section only contains 2 links \n# Data context\nlink to the competition data page\n# Overview of the Approach\nthis section should describe the models or algorithms used, describe the data preprocessing, feature engineering, and/or feature selection strategy, described the validation strategy.\n# Details of the submission\nthis section should include what was special, creative, important, and/or impactful about the submission. And also, what was tried and didn’t work.\n# Sources\nthis section should include links to helpful resources like research papers, past winning write-up solutions, forum posts, helpful notebooks, etc.\"\"\"\n\nexample_messy_writeup = \"\"\"\nThis section only contains 2 links, and here the link to the competition data page.\nPartial section.\nAnother partial section.\nExtensive model secondi which describes the models or algorithms used, describe the data preprocessing, feature engineering, and/or feature selection strategy, described the validation strategy.\nSpecial section should include what was special, creative, important, and/or impactful about the submission. And also, what was tried and didn’t work. Last section should include links to helpful resources like research papers, past winning write-up solutions, forum posts, helpful notebooks, etc.\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:40.928889Z","iopub.execute_input":"2024-03-04T00:05:40.929533Z","iopub.status.idle":"2024-03-04T00:05:40.935339Z","shell.execute_reply.started":"2024-03-04T00:05:40.929496Z","shell.execute_reply":"2024-03-04T00:05:40.934382Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"We'll now use LangChain [text splitters methods](https://python.langchain.com/docs/modules/data_connection/document_transformers/) to demonstrate document splitting. [LangChain](https://python.langchain.com/docs/get_started/introduction) is a framework for developing applications powered by language models and it's great because **simplify tasks** for developing LLM-powered applications, which will easily let us experiment with the above techniques **without re-inventing the wheel.**","metadata":{}},{"cell_type":"code","source":"# Split the clean writeup based on sections\ntext_splitter = CharacterTextSplitter(separator='#', chunk_size=100, chunk_overlap=10)\ntexts_clean_writeup = text_splitter.split_text(example_clean_writeup)\n\n# Print the first characters of each split\nprint([i[:50] for i in texts_clean_writeup])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:40.936505Z","iopub.execute_input":"2024-03-04T00:05:40.936815Z","iopub.status.idle":"2024-03-04T00:05:40.954682Z","shell.execute_reply.started":"2024-03-04T00:05:40.936787Z","shell.execute_reply":"2024-03-04T00:05:40.953871Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"['# Context section\\nThis section only contains 2 lin', 'Data context\\nlink to the competition data page', 'Overview of the Approach\\nthis section should descr', 'Details of the submission\\nthis section should incl', 'Sources\\nthis section should include links to helpf']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Some important parameters to consider:\n- **separator**: the character to split the text on\n- **chunk_size**: number of characters in each chunk\n- **chunk_overlap**: if we want to overlap the current chunk with previous text\n\nWhat `CharacterTextSplitter` does in this example is it first looks for the **first 100 characters** and then **splits the next chunk** from the closest separator.\n\nIn case of a messy writeup, we could opt for a sentence separation:","metadata":{}},{"cell_type":"code","source":"# Split the messy writeup based on newlines\ntext_splitter = CharacterTextSplitter(separator='\\n', chunk_size=100, chunk_overlap=50)\ntexts_messy_writeup = text_splitter.split_text(example_messy_writeup)\n\nprint([i[:50] for i in texts_messy_writeup])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:40.955953Z","iopub.execute_input":"2024-03-04T00:05:40.956272Z","iopub.status.idle":"2024-03-04T00:05:40.966967Z","shell.execute_reply.started":"2024-03-04T00:05:40.956245Z","shell.execute_reply":"2024-03-04T00:05:40.966096Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"['This section only contains 2 links, and here the l', 'Partial section.\\nAnother partial section.', 'Extensive model secondi which describes the models', 'Special section should include what was special, c']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this case, we can see also the effect of the `chunk_overlap`, as the second document includes a `\\n` in its body. This is because we **told the function to overlap** chunks by 50 characters.\n\nThese examples are simplified; actual writeups tend to be more intricate, but **we can leverage HTML formatting**. LangChain comes with `HTMLHeaderTextSplitter`, let's test it using the previous writeup:","metadata":{}},{"cell_type":"code","source":"# Split on HTML headers\nheaders_to_split_on = [\n    (\"h1\", \"Header 1\"),\n    (\"h2\", \"Header 2\")\n]\n\n# Split the real HTML writeup based on headers\ntext_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on, return_each_element=False)\ntexts_html_writeup = text_splitter.split_text(writeup)\n\nprint('Length writup:', len(writeup))\nprint('Number of splits:', len(texts_html_writeup))\nprint('Element returned:', type(texts_html_writeup[0]))\nprint('Length of each split:', [len(i.page_content) for i in texts_html_writeup])\n\n# Print the first characters for each split\nprint(); print([(i.page_content[:50], i.metadata) for i in texts_html_writeup])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:40.970679Z","iopub.execute_input":"2024-03-04T00:05:40.970940Z","iopub.status.idle":"2024-03-04T00:05:41.045861Z","shell.execute_reply.started":"2024-03-04T00:05:40.970917Z","shell.execute_reply":"2024-03-04T00:05:41.045010Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Length writup: 9864\nNumber of splits: 6\nElement returned: <class 'langchain_core.documents.base.Document'>\nLength of each split: [511, 1567, 1040, 1203, 1031, 1260]\n\n[('We used an approach similar to audio spectrogram c', {'Header 2': 'TLDR'}), ('We extracted 18 lip points, 20 pose points (includ', {'Header 2': '1. Data Preprocessing'}), ('These augmentations are used in both CNN training ', {'Header 2': '2. Augmentation'}), ('Train on one fold with a random split (8 folds in ', {'Header 2': '3. Training'}), ('We rewrote all our models in Keras and transferred', {'Header 2': '4. Submissions, Conversion and Ensemble'}), ('Depthwise convolution models performed very well f', {'Header 2': '5. PS. Need BETTER TFlite DepthwiseConv2D'})]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's clarify we we got:\n- The returned structure contains **6 splits** based on HTML headers (h1 and h2, but we can also specify more)\n- Each split is made of a [Document class](https://api.python.langchain.com/en/v0.0.339/schema/langchain.schema.document.Document.html), a specific structure to store text and metadata. As a matter of fact, each Document is made of the **page content and the headers (as metadata).**\n    \nWe could **add the metadata information** back in the page content so that our model will have **access to the section titles for better context.**","metadata":{}},{"cell_type":"code","source":"for i, text in enumerate(texts_html_writeup):\n    # Join the metadata and the content together\n    final_content = '\\n'.join(text.metadata.values()) + '\\n' + text.page_content\n    # Replace the old content with the enriched one\n    text.page_content = final_content\n    \n    # Print some examples\n    if i < 2:\n        print(final_content); print()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:41.047289Z","iopub.execute_input":"2024-03-04T00:05:41.047882Z","iopub.status.idle":"2024-03-04T00:05:41.054731Z","shell.execute_reply.started":"2024-03-04T00:05:41.047857Z","shell.execute_reply":"2024-03-04T00:05:41.053681Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"TLDR\nWe used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.  \nWe used only competition data.\n\n1. Data Preprocessing\nWe extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points. During training, we applied various augmentations. We implemented standard normalization. Instead of dropping NaN values, we filled them with zeros after normalization. We interpolated the time axis to a size of 160 using 'nearest' interpolation: yy = F.interpolate(yy[None, None, :], size=self.new_size, mode='nearest'). Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the (X, Y, Z) axes.  \nOnly 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.  \nAugmentations, normalization and NaN-filling were applied sequentially.  \nSequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.  \nApart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.  \nMotion features consist of future motion and history motion, which can be denoted as:  \n$$ Motion_{future} = position_{t+1} - position_{t} $$ $$ Motion_{history} = position_{t} - position_{t-1} $$  \nFull 210 pairwise distances among 21 hand points were included.  \nThere are 5 vertices in a finger (e.g. thumb is [0,1,2,3,4]), and therefore, there are 3 angles: <0,1,2>, <1,2,3>, <2,3,4>. So 15 angles of 5 fingers were included.  \nRandomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we can could use again `CharacterTextSplitter` if we further want to **split within each document** after a certain chunk size.","metadata":{}},{"cell_type":"code","source":"text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n\n# Split\nsplits = text_splitter.split_documents(texts_html_writeup)\nprint('Number of final splits:', len(splits))\nprint('Length of each final split:', [len(i.page_content) for i in splits])\n\nprint(); print([(i.page_content[:50], i.metadata) for i in splits])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:41.055810Z","iopub.execute_input":"2024-03-04T00:05:41.056126Z","iopub.status.idle":"2024-03-04T00:05:41.064025Z","shell.execute_reply.started":"2024-03-04T00:05:41.056097Z","shell.execute_reply":"2024-03-04T00:05:41.062937Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Number of final splits: 6\nLength of each final split: [516, 1589, 1056, 1215, 1071, 1302]\n\n[('TLDR\\nWe used an approach similar to audio spectrog', {'Header 2': 'TLDR'}), ('1. Data Preprocessing\\nWe extracted 18 lip points, ', {'Header 2': '1. Data Preprocessing'}), ('2. Augmentation\\nThese augmentations are used in bo', {'Header 2': '2. Augmentation'}), ('3. Training\\nTrain on one fold with a random split ', {'Header 2': '3. Training'}), ('4. Submissions, Conversion and Ensemble\\nWe rewrote', {'Header 2': '4. Submissions, Conversion and Ensemble'}), ('5. PS. Need BETTER TFlite DepthwiseConv2D\\nDepthwis', {'Header 2': '5. PS. Need BETTER TFlite DepthwiseConv2D'})]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Given that we selected a character window larger than the document length, we got the same result as before. <br>\n\nNow that everything is clear, let's run some experiment! We'll use the `splits` in a moment with MapReduce and Refine!\n\n<div class=\"alert alert-block alert-info\">\n<b>Key learnings </b><br> <br>\n    - <b>Stuffing, MapReduce and Refine</b> are three different techniques that can be used to summarize documents. <br>\n    - Given the task, document structure and model capabilities, we might need to <b>split our document in chunks</b> to fit the context in our prompt or to improve the summary<br>\n    - Kaggle writeups can potentially all fit in <b>Gemma context length</b>, but different strategies such as Sections splitting based on HTML formatting could potentially be tested\n</div>\n\n---\n\n# Experiments\n\nTo setup our experiments, we'll first wrap our HuggingFace pipeline in LangChain [following this guide](https://python.langchain.com/docs/modules/model_io/llms/custom_llm):","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\ngc.collect()\n\nclass GemmaLLM(LLM):\n    hf_pipe: Any = None\n    pipe_kwargs: Any = None\n        \n    def __init__(self, hf_pipeline, pipe_kwargs):\n        super(GemmaLLM, self).__init__()\n        self.hf_pipe = hf_pipeline\n        self.pipe_kwargs = pipe_kwargs\n\n    @property\n    def _llm_type(self):\n        return \"Gemma pipeline\"\n\n    def _call(self, prompt, **kwargs):\n        \n        outputs = self.hf_pipe(\n            prompt,\n            do_sample=self.pipe_kwargs['do_sample'],\n            temperature=self.pipe_kwargs['temperature'],\n            top_k=self.pipe_kwargs['top_k'],\n            top_p=self.pipe_kwargs['top_p'],\n            add_special_tokens=self.pipe_kwargs['add_special_tokens']\n        )\n        return outputs[0][\"generated_text\"][len(prompt):]  \n\n    @property\n    def _identifying_params(self):\n        \"\"\"Pipeline params\"\"\"\n        return {\"n\": self.pipe_kwargs}\n\nhf = GemmaLLM(hf_pipeline=pipe,\n              pipe_kwargs={\n                    'do_sample':True,\n                    'temperature':0.1,\n                    'top_k':20,\n                    'top_p':0.3,\n                    'add_special_tokens':True\n                })","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:41.065276Z","iopub.execute_input":"2024-03-04T00:05:41.065594Z","iopub.status.idle":"2024-03-04T00:05:41.502910Z","shell.execute_reply.started":"2024-03-04T00:05:41.065566Z","shell.execute_reply":"2024-03-04T00:05:41.502052Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"To see it in action, let's test it on the prompt we created at the beginning:","metadata":{}},{"cell_type":"code","source":"prompt[:350]","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:41.504136Z","iopub.execute_input":"2024-03-04T00:05:41.504423Z","iopub.status.idle":"2024-03-04T00:05:41.517343Z","shell.execute_reply.started":"2024-03-04T00:05:41.504398Z","shell.execute_reply":"2024-03-04T00:05:41.516417Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'<start_of_turn>user\\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\\n\\n<h2>TLDR</h2>\\n<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models s'"},"metadata":{}}]},{"cell_type":"code","source":"out = hf.invoke(prompt)\nprint(out)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:05:41.518822Z","iopub.execute_input":"2024-03-04T00:05:41.519129Z","iopub.status.idle":"2024-03-04T00:06:01.545508Z","shell.execute_reply.started":"2024-03-04T00:05:41.519104Z","shell.execute_reply":"2024-03-04T00:06:01.544553Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Sure, here's a summary of the text in a technical way:\n\n**1. Data Preprocessing**\n\n* Extract 80 points from the image, including lip and pose points.\n* Apply various augmentations and normalizations.\n* Fill NaN values with zeros and use nearest interpolation for the time axis.\n\n**2. Augmentation**\n\n* Use common and CNN specific augmentations.\n* Implement a mixup augmentation that only works with CNNs.\n\n**3. Training**\n\n* Train EfficientNet-B0 and BERT on a single fold with 0.1 warm-up.\n* Train a transformer model with a ranger optimizer and 4-layer transformer.\n* Tune hyperparameters with Optuna.\n\n**4. Submissions**\n\n* Aggregate models in a tf.Module.\n* Calculate ensemble weights for fold 0 and apply to the full dataset.\n\n**5. PS. Need BETTER TFlite DepthwiseConv2D**\n\n* Explore different ways to implement depthwise convolution in tflite.\n* Experiment with different FLOP configurations.\n\n**6. Conclusion**\n\n* EfficientNet-B0 achieved a leaderboard score of 0.8.\n* Transformers improved the score to 0.81.\n* Ensemble of models with different architectures achieved the highest score of 0.82.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Everything is still working as expecting: we wrapped out pipeline and passed the same parameters. Once again, **this was an example of the Stuffing method.**\n\nLet's see how MapReduce and Refine methods perform using the split based on HTML tags we created before.","metadata":{}},{"cell_type":"code","source":"# MapReduce strategy\n\n# Define prompt for summarization of each chunk\nprompt_template = \"\"\"<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n{text}<end_of_turn>\n<start_of_turn>model\"\"\"\nprompt_init = PromptTemplate.from_template(prompt_template)\n\n# Define prompt for final output, the summary of summaries\ncombine_template = \"\"\"<bos><start_of_turn>user\nYou are given a text containing summaries of different part of a document.\nCreate one single summary combining all the information of the chapters. Divide the summary in chapters, be impersonal and use bullet points:\n\n{text}<end_of_turn>\n<start_of_turn>model\"\"\"\ncombine_prompt = PromptTemplate.from_template(combine_template)\n\n# Create the chain of summarization, using map_reduce\nchain = load_summarize_chain(hf, chain_type='map_reduce', map_prompt=prompt_init, combine_prompt=combine_prompt)\n\n# Run the chain on the chunks\nout_summary = chain.invoke(splits)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-04T00:06:01.546977Z","iopub.execute_input":"2024-03-04T00:06:01.547386Z","iopub.status.idle":"2024-03-04T00:07:17.901497Z","shell.execute_reply.started":"2024-03-04T00:06:01.547347Z","shell.execute_reply":"2024-03-04T00:07:17.900680Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d784292a4f9342f99c97c2280ae37332"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c741193ca6134bef83d91fe0ee283fac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23391d50912a4cc687e8e9f69be0ea8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe6a025ff83249019dc2eeef9e5ae45a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"500e4b77602445ceb0852d07ab86076c"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1749 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"print(out_summary['output_text'].replace('\\n\\n','\\n'))","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:07:17.902663Z","iopub.execute_input":"2024-03-04T00:07:17.902961Z","iopub.status.idle":"2024-03-04T00:07:17.907777Z","shell.execute_reply.started":"2024-03-04T00:07:17.902934Z","shell.execute_reply":"2024-03-04T00:07:17.906965Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":" 1: EfficientNet-B0\n* The EfficientNet-B0 model is a deep neural network architecture that is designed to be efficient.\n* The model consists of a series of depthwise convolutions, followed by a global average pooling layer.\n* The model is trained using a single fold of 8 randomly split folds.\n**Chapter 1: Data Preparation**\n* The dataset consists of 10,000 images with 10 classes.\n* The EfficientNet-B0 model is trained on a single fold with the following settings:\n    * Input size: 160x80\n    * Number of filters: 512\n    * Number of layers: 19\n    * Batch size: 32\n    * Learning rate: 0.001\n**Chapter 2: Model Training**\n* The EfficientNet-B0 model is trained on the single fold with the following settings:\n    * Input size: 160x80\n    * Number of filters: 512\n    * Number of layers: 19\n    * Batch size: 32\n    * Learning rate: 0.001\n**Chapter 3: Evaluation**\n* The model is evaluated on the single fold with the following metrics:\n    * CV score: 0.898\n    * Leaderboard score: ~0.8\n**Chapter 4: Results and Discussion**\n* The EfficientNet-B0 model with the single fold training achieves a CV score of 0.898.\n* The EfficientNet-B0 model with the single fold training achieves a leaderboard score of ~0.8.\n* The model is trained on only competition data, which may explain its high performance.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Considering that we divided our writeup in chunks, the results is still decent. We lost some coherence as the model can't access the entire document at once. <br>\nBy setting `verbose=true`, we can also see the steps of the chain (**expand the output cell!**):","metadata":{}},{"cell_type":"code","source":"# Repeat the process above, with verbose True\nchain = load_summarize_chain(hf, chain_type='map_reduce', verbose=True, map_prompt=prompt_init, combine_prompt=combine_prompt)\n\n# Run the chain on the chunks\nout_summary = chain.invoke(splits)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-04T00:07:17.908879Z","iopub.execute_input":"2024-03-04T00:07:17.909211Z","iopub.status.idle":"2024-03-04T00:08:32.765096Z","shell.execute_reply.started":"2024-03-04T00:07:17.909186Z","shell.execute_reply":"2024-03-04T00:08:32.763947Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n\n\n\u001b[1m> Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\nTLDR\nWe used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.  \nWe used only competition data.<end_of_turn>\n<start_of_turn>model\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n1. Data Preprocessing\nWe extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points. During training, we applied various augmentations. We implemented standard normalization. Instead of dropping NaN values, we filled them with zeros after normalization. We interpolated the time axis to a size of 160 using 'nearest' interpolation: yy = F.interpolate(yy[None, None, :], size=self.new_size, mode='nearest'). Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the (X, Y, Z) axes.  \nOnly 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.  \nAugmentations, normalization and NaN-filling were applied sequentially.  \nSequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.  \nApart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.  \nMotion features consist of future motion and history motion, which can be denoted as:  \n$$ Motion_{future} = position_{t+1} - position_{t} $$ $$ Motion_{history} = position_{t} - position_{t-1} $$  \nFull 210 pairwise distances among 21 hand points were included.  \nThere are 5 vertices in a finger (e.g. thumb is [0,1,2,3,4]), and therefore, there are 3 angles: <0,1,2>, <1,2,3>, <2,3,4>. So 15 angles of 5 fingers were included.  \nRandomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.<end_of_turn>\n<start_of_turn>model\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n2. Augmentation\nThese augmentations are used in both CNN training and transformer training  \nRandom affine: Same as @hengck23 shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).  \nRandom interpolation: Slightly scale and shift the time dimension.  \nFlip pose: Flip the x-coordinates of all points. In CNN, x_new = x_max - x_old. In transformer, x_new = 2 * frame[:,0,0] - x_old.  \nFinger tree rotate: There are 4 root-children pairs in a finger with 5-vertices. E.g. in thumb ([0,1,2,3,4]), these 4 root-children pairs are: 0-[1,2,3,4],1-[2,3,4],2-[3,4],3-[4]. We randomly choose some of these pairs, and rotate the children points around root point with a small random angle.  \nMixup: Implement basic mixup augmentation (only works with CNNs, not transformers). Replace augmentation: Replace some random parts from other samples of the same class. Time and frequence masking: This basic torchaudio augmentation works exceptionally well.  \nBefore augmentation:  \nAfter augmentation:<end_of_turn>\n<start_of_turn>model\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n3. Training\nTrain on one fold with a random split (8 folds in total) or the full dataset using the best parameters Onecycle scheduler with 0.1 warmup. Use weighted CrossEntropyLoss. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat) Implement a hypercolumn for EfficientNet with 5 blocks  \nTrain on one fold with a random split (8 folds in total) or the full dataset using the best parameters Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule. A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained. A 3-layer model was initialized with 4-layer model's first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.  \nSince we trained only one fold and used smaller models, we decided to tune most parameters with Optuna.  \nHere is the parameters list of CNN training (transformer training has a similar param-list):  \nAll augmentations probabilities (0.1 - 0.5+)  \nLearning rate (2e-3 - 3e-3)  \nDrop out (0.1 - 0.25)  \nNum of epochs (170-185)  \nLoss weights powers (0.75 - 2)  \nOptimizer (Lookahead_RAdam, RAdam)  \nLabel smoothing (0.5 - 0.7)<end_of_turn>\n<start_of_turn>model\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n4. Submissions, Conversion and Ensemble\nWe rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%. For transformer model, pytorch-onnx-tf-tflite will generate too much useless tensor shape operations, a fully rewriting can reduce these manually. For CNN model, we rewrote DepthwiseConv2D with a hard-coded way, whose speed is 200%~300% of its original version of tflite DepthwiseConv2D.  \nAfter that, we aggregated all these models in the tf.Module class. Converting directly from Keras resulted in lower speed (don't know why).  \nWe calculated ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.  \nEfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81. The final ensemble included:  \nEfficientnet-B0, fold 0 BERT, full data train DeBERTa, full data train  \nInterestingly, a key feature was using the ensemble without softmax, which consistently provided a boost of around 0.01.<end_of_turn>\n<start_of_turn>model\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n5. PS. Need BETTER TFlite DepthwiseConv2D\nDepthwise convolution models performed very well for these tasks, outperforming other CNN and ViT models (rexnet_100 was also good). We spent a lot of time dealing with the conversion of DepthwiseConv2D operation. Here are some strange results:  \nGiven a input image with 82x42x32 (HWC), there are two ways to do a 3x3 depthwise convolution in Keras. One is Conv2D(32, 3, groups = 32), the other is DepthwiseConv2D(3). However, after converting these two to tflite, the running time of the Conv2D is 5.05ms, and the running time of DepthwiseConv2D is 3.70ms. More strangely, a full convolution Conv2D(32, 3, groups = 1) with FLOPs = HWC^2 only takes 2.09ms, even faster than previous two with FLOPs = HWC.  \nThen we rewrote the depthwise-conv like this:  \nThe running time of this is 1.24 ms.  \nIn summary, our version (1.24ms) > full Conv2D with larger FLOPs (2.09ms) > DepthwiseConv2D (3.70ms) > Conv2D(C, groups = C) (5.05ms).  \nHowever, our version introduced too much nodes in tflite graph, which is not stable in running time. If the tensorflow team has a better implementation of DepthwiseConv2D, we can even ensemble two CNN models, which is expected to reach 0.82 LB.  \nBy the way, EfficientNet with ONNX was ~5 times faster than TFLite.  \ngithub code<end_of_turn>\n<start_of_turn>model\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n\n\n\u001b[1m> Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\nYou are given a text containing summaries of different part of a document.\nCreate one single summary combining all the information of the chapters. Divide the summary in chapters, be impersonal and use bullet points:\n\n architecture:\n\n* EfficientNet-B0 model\n* Transformer models (BERT and DeBERTa) as helper models\n* Single fold training with 8 splits\n* Use of competition data only\n</start_of_turn>\n\n**Chapter 1: Data Preparation**\n\n* The dataset consists of 10,000 images with 10 classes.\n* The EfficientNet-B0 model is used as the base model.\n* The BERT and DeBERTa models are used as helper models.\n* The model is trained on a single fold from 8 randomly split folds.\n\n**Chapter 2: Model Training**\n\n* The EfficientNet-B0 model is trained on the single fold with the following settings:\n    * Input size: 160x80\n    * Number of filters: 512\n    * Number of layers: 19\n    * Batch size: 32\n    * Learning rate: 0.001\n* The BERT and DeBERTa models are trained on the full dataset with the following settings:\n    * Input size: 128\n    * Number of filters: 512\n    * Number of layers: 12\n    * Batch size: 16\n    * Learning rate: 0.001\n\n**Chapter 3: Evaluation**\n\n* The model is evaluated on the single fold with the following metrics:\n    * CV score: 0.898\n    * Leaderboard score: ~0.8\n\n**Chapter 4: Results and Discussion**\n\n* The EfficientNet-B0 model with the single fold training achieves a CV score of 0.898.\n* The BERT and DeBERTa models achieve a leaderboard score of ~0.8.\n* The model is trained on only competition data, which may explain its high performance.\n\n 1: Data Preprocessing\n\n- Extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points.\n- Applied various augmentations.\n- Implemented standard normalization.\n- Filled NaN values with zeros after normalization.\n- Interpolated the time axis to a size of 160 using 'nearest' interpolation.\n- Obtained a tensor with dimensions 160x80x3.\n\n<start_of_turn>model 2: Feature Extraction\n\n- Hand-crafted features were also used, including motion, distances, and cosine of angles.\n\n<start_of_turn>model 3: Feature Engineering\n\n- Motion features consist of future motion and history motion.\n- Full 210 pairwise distances among 21 hand points were included.\n- 15 angles of 5 fingers were included.\n- Randomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.\n\n:\n**Chapter 1: Augmentation Techniques**\n\n* **Random affine:** Randomly shifts and scales each part of the image.\n* **Random interpolation:** Randomly scales and shifts the time dimension of the image.\n* **Flip pose:** Flip the x-coordinates of all points in the image.\n* **Finger tree rotate:** Randomly rotates the children points of the finger tree.\n* **Mixup:** Replace some random parts from other samples of the same class.\n\n**Chapter 2: Time and Frequency Masking**\n\n* This technique is used to increase the diversity of the training data.\n* It involves randomly masking out parts of the image and then replacing them with a different part of the image.\n* This technique is effective in reducing overfitting and improving thegeneralizability of the model.\n\n architecture:\n\n- Input size: 224x224x3\n- Output size: 768\n- Number of filters: 512\n- Number of hidden units: 256\n- Number of layers: 4\n- Activation function: ReLU\n- Loss function: Weighted CrossEntropyLoss\n- Optimizer: Lookahead_RAdam\n- Learning rate: 2e-3 - 3e-3\n- Dropout: 0.1 - 0.25\n- Number of epochs: 170-185\n- Loss weight power: 0.75 - 2\n- Number of classes: 10\n\n**Chapter 1: Training**\n\n- Use a random split of the data to divide the dataset into 8 folds.\n- Train the model on each fold with the best parameters found by Optuna.\n- Use weighted CrossEntropyLoss to penalize misclassification.\n- Increase the weights for poorly predicted classes and classes with semantically similar pairs.\n\n**Chapter 2: Model Architecture**\n\n- Use a 4-layer, 256 hidden-size, 512 intermediate-size transformer.\n- Initialize the 3-layer model with the first 3 layers of the 4-layer model.\n- Use knowledge distillation to transfer knowledge from the 4-layer model to the 3-layer model.\n\n**Chapter 3: Optimization**\n\n- Use Optuna to tune most parameters of the model.\n- Set the learning rate, drop out, epochs, and loss weight power as hyperparameters.\n- Use the Lookahead_RAdam optimizer with a learning rate schedule.\n- Use the Weighted CrossEntropyLoss function.\n\n_summary\n\n**Chapter 1: Model Rewriting**\n\n* Rewrote all models in Keras and transferred PyTorch weights to them.\n* For transformer model, pytorch-onnx-tf-tflite generated too much useless tensor shape operations.\n* For CNN model, DepthwiseConv2D was rewritten with a hard-coded way, whose speed was 200%~300% of its original version of tflite DepthwiseConv2D.\n\n**Chapter 2: Ensemble**\n\n* Calculated ensemble weights for models trained on fold 0 using the local fold 0 score.\n* Applied these weights to the full dataset models.\n\n**Chapter 3: Results**\n\n* EfficientNet-B0 achieved a leaderboard score of approximately 0.8.\n* Transformers improved the score to 0.81.\n* The final ensemble included Efficientnet-B0, fold 0 BERT, full data train DeBERTa, and full data train.\n\n summary:\n\n**Chapter 1: Introduction**\n\n* Depthwise convolution is a type of convolution that is used to extract features from images.\n* Traditional CNN and ViT models are both used for depthwise convolution, but they can be computationally expensive.\n* The goal of this paper is to compare the performance of different depthwise convolution implementations.\n\n**Chapter 2: Results**\n\n* The Conv2D operation was found to be the most efficient depthwise convolution implementation.\n* DepthwiseConv2D with 1 group was the fastest of the three methods tested, with a running time of 1.24 ms.\n* The full Conv2D operation with 32 groups took 2.09 ms, while the DepthwiseConv2D operation with 32 groups took 3.70 ms.\n* The EfficientNet model with ONNX was ~5 times faster than TFLite.\n\n**Chapter 3: Discussion**\n\n* The results show that DepthwiseConv2D with 1 group is the most efficient depthwise convolution implementation.\n* This is likely due to the fact that DepthwiseConv2D with 1 group is able to use a smaller kernel size, which can reduce the number of parameters that need to be learned.\n* The results also show that the running time of DepthwiseConv2D can be reduced by using a smaller kernel size.\n* The results suggest that the tensorflow team should consider improving the performance of DepthwiseConv2D.<end_of_turn>\n<start_of_turn>model\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's see the **Refine method** in action!","metadata":{}},{"cell_type":"code","source":"# Refine strategy \n\n# Define prompt for the first summarization\nprompt_template = \"\"\"<bos><start_of_turn>user\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n\n{text}<end_of_turn>\n<start_of_turn>model\"\"\"\nprompt_init = PromptTemplate.from_template(prompt_template)\n\n# Define prompt for the refine phase, enhancing the previous summary with the new information\nrefine_template = \"\"\"<bos><start_of_turn>user\nYour job is to produce a final document divided in chapters and bullet points.\nYou are given a text containing an existing summary to a certain point:\n\n{existing_answer}\n\nYou can now refine it (if necessary) with more context below.\n\n{text}\n\nGiven the new context, refine the original summary.<end_of_turn>\n<start_of_turn>model\"\"\"\nprompt_refine = PromptTemplate.from_template(refine_template)\n\n\nchain = load_summarize_chain(hf, chain_type='refine',\n                             return_intermediate_steps=True,\n                             input_key='input_documents',\n                             output_key='output_text',\n                             question_prompt=prompt_init,\n                             refine_prompt=prompt_refine)\n\nout_summary = chain.invoke(splits, return_only_outputs=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:08:32.766187Z","iopub.execute_input":"2024-03-04T00:08:32.766463Z","iopub.status.idle":"2024-03-04T00:10:05.180169Z","shell.execute_reply.started":"2024-03-04T00:08:32.766438Z","shell.execute_reply":"2024-03-04T00:10:05.179331Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print(out_summary['output_text'])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:10:05.181372Z","iopub.execute_input":"2024-03-04T00:10:05.181659Z","iopub.status.idle":"2024-03-04T00:10:05.186727Z","shell.execute_reply.started":"2024-03-04T00:10:05.181634Z","shell.execute_reply":"2024-03-04T00:10:05.185862Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":" training\n\nThe EfficientNet-B0 model is trained on the single fold with the following settings:\n\n* Input size: 160x80\n* Number of filters: 512\n* Number of layers: 19\n* Batch size: 32\n* Learning rate: 0.001\n\nThe BERT and DeBERTa models are trained on the full dataset with the following settings:\n\n* Input size: 128\n* Number of filters: 512\n* Number of layers: 12\n* Batch size: 16\n* Learning rate: 0.001\n\n**Chapter 2: Model Training**\n\n* The EfficientNet-B0 model is trained on the single fold with the following settings:\n    * Input size: 160x80\n    * Number of filters: 512\n    * Number of layers: 19\n    * Batch size: 32\n    * Learning rate: 0.001\n* The BERT and DeBERTa models are trained on the full dataset with the following settings:\n    * Input size: 128\n    * Number of filters: 512\n    * Number of layers: 12\n    * Batch size: 16\n    * Learning rate: 0.001\n\n**Chapter 3: Evaluation**\n\nThe model is evaluated on the validation set with the following metrics:\n\n* Accuracy\n* Precision\n* Recall\n* F1-score\n\n**Chapter 4: Results and Discussion**\n\nThe results of the model training and evaluation are presented in the following table:\n\n| Metric | EfficientNet-B0 | BERT | DeBERTa |\n|---|---|---|---|\n| Accuracy | 95% | 97% | 98% |\n| Precision | 98% | 99% | 100% |\n| Recall | 94% | 96% | 97% |\n| F1-score | 96% | 98% | 99% |\n\n**Chapter 5: Conclusion**\n\nThe EfficientNet-B0 model achieves the highest accuracy, precision, recall, and F1-score among the three models.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can still set `verbose=True` as before, or we can directly inspect the intermediate steps created (**expand the cell output**):","metadata":{}},{"cell_type":"code","source":"print(\"\\n###############################\\n\".join(out_summary[\"intermediate_steps\"]))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-04T00:10:05.187928Z","iopub.execute_input":"2024-03-04T00:10:05.188244Z","iopub.status.idle":"2024-03-04T00:10:05.201182Z","shell.execute_reply.started":"2024-03-04T00:10:05.188221Z","shell.execute_reply":"2024-03-04T00:10:05.200280Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":" architecture:\n\n* EfficientNet-B0 model\n* Transformer models (BERT and DeBERTa) as helper models\n* Single fold training with 8 splits\n* Use of competition data only\n</start_of_turn>\n\n**Chapter 1: Data Preparation**\n\n* The dataset consists of 10,000 images with 10 classes.\n* The EfficientNet-B0 model is used as the base model.\n* The BERT and DeBERTa models are used as helper models.\n* The model is trained on a single fold from 8 randomly split folds.\n\n**Chapter 2: Model Training**\n\n* The EfficientNet-B0 model is trained on the single fold with the following settings:\n    * Input size: 160x80\n    * Number of filters: 512\n    * Number of layers: 19\n    * Batch size: 32\n    * Learning rate: 0.001\n* The BERT and DeBERTa models are trained on the full dataset with the following settings:\n    * Input size: 128\n    * Number of filters: 512\n    * Number of layers: 12\n    * Batch size: 16\n    * Learning rate: 0.001\n\n**Chapter 3: Evaluation**\n\n* The model is evaluated on the single fold with the following metrics:\n    * CV score: 0.898\n    * Leaderboard score: ~0.8\n\n**Chapter 4: Results and Discussion**\n\n* The EfficientNet-B0 model with the single fold training achieves a CV score of 0.898.\n* The BERT and DeBERTa models achieve a leaderboard score of ~0.8.\n* The model is trained on only competition data, which may explain its high performance.\n###############################\n training\n\n* The EfficientNet-B0 model is used as the base model.\n* The BERT and DeBERTa models are used as helper models.\n* The model is trained on a single fold from 8 randomly split folds.\n\n**Chapter 2: Model Training**\n\n* The EfficientNet-B0 model is trained on the single fold with the following settings:\n    * Input size: 160x80\n    * Number of filters: 512\n    * Number of layers: 19\n    * Batch size: 32\n    * Learning rate: 0.001\n* The BERT and DeBERTa models are trained on the full dataset with the following settings:\n    * Input size: 128\n    * Number of filters: 512\n    * Number of layers: 12\n    * Batch size: 16\n    * Learning rate: 0.001\n###############################\n training\n\nThe EfficientNet-B0 model is trained on the single fold with the following settings:\n\n* Input size: 160x80\n* Number of filters: 512\n* Number of layers: 19\n* Batch size: 32\n* Learning rate: 0.001\n\nThe BERT and DeBERTa models are trained on the full dataset with the following settings:\n\n* Input size: 128\n* Number of filters: 512\n* Number of layers: 12\n* Batch size: 16\n* Learning rate: 0.001\n\n**Chapter 2: Model Training**\n\n* The EfficientNet-B0 model is trained on the single fold with the following settings:\n    * Input size: 160x80\n    * Number of filters: 512\n    * Number of layers: 19\n    * Batch size: 32\n    * Learning rate: 0.001\n* The BERT and DeBERTa models are trained on the full dataset with the following settings:\n    * Input size: 128\n    * Number of filters: 512\n    * Number of layers: 12\n    * Batch size: 16\n    * Learning rate: 0.001\n\n**Chapter 3: Evaluation**\n\nThe model is evaluated on the validation set with the following metrics:\n\n* Accuracy\n* Precision\n* Recall\n* F1-score\n\n**Chapter 4: Results and Discussion**\n\nThe results of the model training and evaluation are presented in the following table:\n\n| Metric | EfficientNet-B0 | BERT | DeBERTa |\n|---|---|---|---|\n| Accuracy | 95% | 97% | 98% |\n| Precision | 98% | 99% | 100% |\n| Recall | 94% | 96% | 97% |\n| F1-score | 96% | 98% | 99% |\n\n**Chapter 5: Conclusion**\n\nThe EfficientNet-B0 model achieves the highest accuracy, precision, recall, and F1-score among the three models.\n###############################\n training\n\nThe EfficientNet-B0 model is trained on the single fold with the following settings:\n\n* Input size: 160x80\n* Number of filters: 512\n* Number of layers: 19\n* Batch size: 32\n* Learning rate: 0.001\n\nThe BERT and DeBERTa models are trained on the full dataset with the following settings:\n\n* Input size: 128\n* Number of filters: 512\n* Number of layers: 12\n* Batch size: 16\n* Learning rate: 0.001\n\n**Chapter 2: Model Training**\n\n* The EfficientNet-B0 model is trained on the single fold with the following settings:\n    * Input size: 160x80\n    * Number of filters: 512\n    * Number of layers: 19\n    * Batch size: 32\n    * Learning rate: 0.001\n* The BERT and DeBERTa models are trained on the full dataset with the following settings:\n    * Input size: 128\n    * Number of filters: 512\n    * Number of layers: 12\n    * Batch size: 16\n    * Learning rate: 0.001\n\n**Chapter 3: Evaluation**\n\nThe model is evaluated on the validation set with the following metrics:\n\n* Accuracy\n* Precision\n* Recall\n* F1-score\n\n**Chapter 4: Results and Discussion**\n\nThe results of the model training and evaluation are presented in the following table:\n\n| Metric | EfficientNet-B0 | BERT | DeBERTa |\n|---|---|---|---|\n| Accuracy | 95% | 97% | 98% |\n| Precision | 98% | 99% | 100% |\n| Recall | 94% | 96% | 97% |\n| F1-score | 96% | 98% | 99% |\n\n**Chapter 5: Conclusion**\n\nThe EfficientNet-B0 model achieves the highest accuracy, precision, recall, and F1-score among the three models.\n###############################\n training\n\nThe EfficientNet-B0 model is trained on the single fold with the following settings:\n\n* Input size: 160x80\n* Number of filters: 512\n* Number of layers: 19\n* Batch size: 32\n* Learning rate: 0.001\n\nThe BERT and DeBERTa models are trained on the full dataset with the following settings:\n\n* Input size: 128\n* Number of filters: 512\n* Number of layers: 12\n* Batch size: 16\n* Learning rate: 0.001\n\n**Chapter 2: Model Training**\n\n* The EfficientNet-B0 model is trained on the single fold with the following settings:\n    * Input size: 160x80\n    * Number of filters: 512\n    * Number of layers: 19\n    * Batch size: 32\n    * Learning rate: 0.001\n* The BERT and DeBERTa models are trained on the full dataset with the following settings:\n    * Input size: 128\n    * Number of filters: 512\n    * Number of layers: 12\n    * Batch size: 16\n    * Learning rate: 0.001\n\n**Chapter 3: Evaluation**\n\nThe model is evaluated on the validation set with the following metrics:\n\n* Accuracy\n* Precision\n* Recall\n* F1-score\n\n**Chapter 4: Results and Discussion**\n\nThe results of the model training and evaluation are presented in the following table:\n\n| Metric | EfficientNet-B0 | BERT | DeBERTa |\n|---|---|---|---|\n| Accuracy | 95% | 97% | 98% |\n| Precision | 98% | 99% | 100% |\n| Recall | 94% | 96% | 97% |\n| F1-score | 96% | 98% | 99% |\n\n**Chapter 5: Conclusion**\n\nThe EfficientNet-B0 model achieves the highest accuracy, precision, recall, and F1-score among the three models.\n###############################\n training\n\nThe EfficientNet-B0 model is trained on the single fold with the following settings:\n\n* Input size: 160x80\n* Number of filters: 512\n* Number of layers: 19\n* Batch size: 32\n* Learning rate: 0.001\n\nThe BERT and DeBERTa models are trained on the full dataset with the following settings:\n\n* Input size: 128\n* Number of filters: 512\n* Number of layers: 12\n* Batch size: 16\n* Learning rate: 0.001\n\n**Chapter 2: Model Training**\n\n* The EfficientNet-B0 model is trained on the single fold with the following settings:\n    * Input size: 160x80\n    * Number of filters: 512\n    * Number of layers: 19\n    * Batch size: 32\n    * Learning rate: 0.001\n* The BERT and DeBERTa models are trained on the full dataset with the following settings:\n    * Input size: 128\n    * Number of filters: 512\n    * Number of layers: 12\n    * Batch size: 16\n    * Learning rate: 0.001\n\n**Chapter 3: Evaluation**\n\nThe model is evaluated on the validation set with the following metrics:\n\n* Accuracy\n* Precision\n* Recall\n* F1-score\n\n**Chapter 4: Results and Discussion**\n\nThe results of the model training and evaluation are presented in the following table:\n\n| Metric | EfficientNet-B0 | BERT | DeBERTa |\n|---|---|---|---|\n| Accuracy | 95% | 97% | 98% |\n| Precision | 98% | 99% | 100% |\n| Recall | 94% | 96% | 97% |\n| F1-score | 96% | 98% | 99% |\n\n**Chapter 5: Conclusion**\n\nThe EfficientNet-B0 model achieves the highest accuracy, precision, recall, and F1-score among the three models.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see how the summary **progressively grows** as new context is being passed to the previous step.\n\nConsidering the current capabilities of Gemma and the length of the writeup. I would likely choose **the Stuffing strategy**. However, with some prompt tuning, the **other methods could be a viable approach.**\n\n<div class=\"alert alert-block alert-warning\">\nMapReduce and Refine <b>heavily depends on the prompts you provide!</b>\n</div>\n\n<div class=\"alert alert-block alert-info\">\n<b>Key learnings </b><br><br>\n    - Dividing in <b>chunk based on HTML sections seems</b> to be a reasonable approach to preserve the logic of the document.<br>\n    - Gemma is able to handle large context window, therefore <b>Stuffing summarization</b> seems the go-to strategy for Kaggle writeups. <br>\n    - <b>Refine method is a valid alternative</b> given the progressive growth of the final output\n</div>\n\n---\n\n# Fine-tuning Gemma with LoRa\n\nEarlier we saw that we can achieve **great results by optimizing our prompts**. However, there might be times where [fine-tuning a model would work better](https://huggingface.co/docs/transformers/tasks/prompting#prompting-vs-fine-tuning):\n\n- The domain is wildly different from what LLMs were pre-trained on and prompt optimization did not yield sufficient results.\n- We need our model to work well in a low-resource language.\n- We need the model to be trained on sensitive data that is under strict regulations.\n- We have to use a small model due to cost, privacy, infrastructure or other limitations.\n\nIn such scenarios, we will need to fine-tuned our model on a **domain-specific dataset**. <br> \n\nFine-tuning a LLM is **computational demanding**. In order to **train it on commercial laptops**, the only feasible way to save memory is by **reducing the model size.** Building on this, the idea is to **fine-tune only few parameters** rather than the entire model, a method currently used extensively called **Parameter Efficient Fine-Tuning or PEFT.**\nFor more details about PEFT and, specifically, **LoRa**, the technique we are going to use now, I encourage you to read this beautiful [blog post](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-tune-an-LLM-Part-3-The-HuggingFace-Trainer--Vmlldzo1OTEyNjMy#parameter-efficient-fine-tuning-(peft)).\n\nLet's now see how **Gemma can be fine-tuned** using HuggingFace [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index) and [datasets](https://huggingface.co/docs/datasets/index). \n\nTo do so, I'll use the [CNN daily news dataset](https://huggingface.co/datasets/cnn_dailymail#dataset-card-for-cnn-dailymail-dataset), also available on Kaggle. This dataset comprises **news articles written by journalists at CNN and the Daily Mail**. For each instance, there is a string for the article and a **string for the highlights (summary).**\n\n<div class=\"alert alert-block alert-warning\">\nCurrently, there are <b> no domain-specific dataset </b> to fine-tune our models on <b>Kaggle writeups </b>. <br> We will utilize the CNN news dataset simply to <b>demonstrate the benefits of fine-tuning which could potentially apply to our specific use case as well</b>.\n</div>","metadata":{}},{"cell_type":"code","source":"# Make space in memory\nhf = release_memory(hf)\n\nwith torch.no_grad():\n    torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:10:05.202282Z","iopub.execute_input":"2024-03-04T00:10:05.202620Z","iopub.status.idle":"2024-03-04T00:10:06.108775Z","shell.execute_reply.started":"2024-03-04T00:10:05.202589Z","shell.execute_reply":"2024-03-04T00:10:06.107793Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# Import of the validation set which contains fewer examples than training\nvalidation = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv')[['article', 'highlights']]\nvalidation.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:10:06.110058Z","iopub.execute_input":"2024-03-04T00:10:06.110784Z","iopub.status.idle":"2024-03-04T00:10:07.337617Z","shell.execute_reply.started":"2024-03-04T00:10:06.110747Z","shell.execute_reply":"2024-03-04T00:10:07.336624Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                             article  \\\n0  Sally Forrest, an actress-dancer who graced th...   \n1  A middle-school teacher in China has inked hun...   \n2  A man convicted of killing the father and sist...   \n3  Avid rugby fan Prince Harry could barely watch...   \n4  A Triple M Radio producer has been inundated w...   \n\n                                          highlights  \n0  Sally Forrest, an actress-dancer who graced th...  \n1  Works include pictures of Presidential Palace ...  \n2  Iftekhar Murtaza, 29, was convicted a year ago...  \n3  Prince Harry in attendance for England's crunc...  \n4  Nick Slater's colleagues uploaded a picture to...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>highlights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sally Forrest, an actress-dancer who graced th...</td>\n      <td>Sally Forrest, an actress-dancer who graced th...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A middle-school teacher in China has inked hun...</td>\n      <td>Works include pictures of Presidential Palace ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A man convicted of killing the father and sist...</td>\n      <td>Iftekhar Murtaza, 29, was convicted a year ago...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Avid rugby fan Prince Harry could barely watch...</td>\n      <td>Prince Harry in attendance for England's crunc...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Triple M Radio producer has been inundated w...</td>\n      <td>Nick Slater's colleagues uploaded a picture to...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's now setup the LoRa [configurations](https://huggingface.co/blog/gemma-peft#low-rank-adaptation-for-large-language-models), mainly following this [blog post](https://huggingface.co/blog/gemma-peft#low-rank-adaptation-for-large-language-models) from HuggingFace.","metadata":{}},{"cell_type":"code","source":"model = \"/kaggle/input/gemma/transformers/2b-it/1\"\n\nlora_config = LoraConfig(\n    r=6,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model)\ntokenizer.padding_side = \"right\" # Fixing overflow issue ref: source code\nmodel = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=bnb_config)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:10:07.338770Z","iopub.execute_input":"2024-03-04T00:10:07.339046Z","iopub.status.idle":"2024-03-04T00:10:13.622753Z","shell.execute_reply.started":"2024-03-04T00:10:07.339022Z","shell.execute_reply":"2024-03-04T00:10:13.622011Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7265c93dfdf942c3a3dedaed2f06a2ea"}},"metadata":{}}]},{"cell_type":"markdown","source":"Once again, we need to pay attention on how we are going to [format our examples](https://huggingface.co/docs/trl/sft_trainer#dataset-format-support) (remember the chat template?). This time, we make sure we are not generating the model response at the end of the string by setting `add_generation_prompt = False`:","metadata":{}},{"cell_type":"code","source":"train_data = Dataset.from_pandas(validation)\n\ndef formatting_prompts_func(example):\n    output_texts = []\n    for i in range(len(example['article'])):\n        messages = [\n            {\"role\": \"user\",\n             \"content\": \"Given the following article, write a short summary of the article in 2-3 sentences:\\n\\nArticle: {}\".format(example['article'][i])},\n            {\"role\": \"assistant\",\n             \"content\": \"{}\".format(example['highlights'][i])}\n        ]\n        output_texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False))\n        \n    return output_texts\n\n# Print the first training example\nprint(formatting_prompts_func(train_data[:1])[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:10:13.623850Z","iopub.execute_input":"2024-03-04T00:10:13.624145Z","iopub.status.idle":"2024-03-04T00:10:14.123703Z","shell.execute_reply.started":"2024-03-04T00:10:13.624120Z","shell.execute_reply":"2024-03-04T00:10:14.122635Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"<start_of_turn>user\nGiven the following article, write a short summary of the article in 2-3 sentences:\n\nArticle: Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .<end_of_turn>\n<start_of_turn>model\nSally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films died on March 15 .\nForrest, whose birth name was Katherine Feeney, had long battled cancer .\nA San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .<end_of_turn>\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we can launch the actual training phase:","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    max_seq_length=512,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=12,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        report_to='none',\n        output_dir='logs',\n        optim=\"paged_adamw_8bit\"\n    ),\n    peft_config=lora_config,\n    formatting_func=formatting_prompts_func,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:10:14.124901Z","iopub.execute_input":"2024-03-04T00:10:14.125271Z","iopub.status.idle":"2024-03-04T00:11:34.098629Z","shell.execute_reply.started":"2024-03-04T00:10:14.125237Z","shell.execute_reply":"2024-03-04T00:11:34.097735Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a12834f1cc24e51b2b6264a0d97e602"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12/12 00:50, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.387200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.499900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.503000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.583300</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.074000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.088500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>3.142500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.115700</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.570400</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.756600</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.917700</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.563300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=12, training_loss=3.100179115931193, metrics={'train_runtime': 55.0034, 'train_samples_per_second': 0.873, 'train_steps_per_second': 0.218, 'total_flos': 288551021051904.0, 'train_loss': 3.100179115931193, 'epoch': 0.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model.save_pretrained('lora_adapter')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:11:34.099845Z","iopub.execute_input":"2024-03-04T00:11:34.100544Z","iopub.status.idle":"2024-03-04T00:11:34.204325Z","shell.execute_reply.started":"2024-03-04T00:11:34.100505Z","shell.execute_reply":"2024-03-04T00:11:34.203535Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"We now have saved the LoRa adapter which can be later be used to either load the model or continue training if necessary.","metadata":{}},{"cell_type":"code","source":"# Make space in memory\ntrainer, model, tokenizer = release_memory(trainer, model, tokenizer)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-04T00:11:34.209506Z","iopub.execute_input":"2024-03-04T00:11:34.210243Z","iopub.status.idle":"2024-03-04T00:11:34.797075Z","shell.execute_reply.started":"2024-03-04T00:11:34.210205Z","shell.execute_reply":"2024-03-04T00:11:34.795872Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"We could continue with our model, but for the sake of this tutorial I'll show how to [load our adapter and merge into Gemma pretrained model](https://huggingface.co/docs/trl/use_model#use-adapters-peft).","metadata":{}},{"cell_type":"code","source":"# Load the pretrained model and our LoRa adapter\nbase_model_name = \"/kaggle/input/gemma/transformers/2b-it/1\"\nadapter_model_name = \"/kaggle/working/lora_adapter\"\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, device_map='auto', torch_dtype=torch.float16)\nmodel = PeftModel.from_pretrained(model, adapter_model_name, device_map='auto', torch_dtype=torch.float16)\n\n# Merge the adapters into the base model so you can use the model like a normal transformers model\nmodel = model.merge_and_unload()\nmodel.save_pretrained('final_model')\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:11:34.798291Z","iopub.execute_input":"2024-03-04T00:11:34.798574Z","iopub.status.idle":"2024-03-04T00:11:52.032220Z","shell.execute_reply.started":"2024-03-04T00:11:34.798550Z","shell.execute_reply":"2024-03-04T00:11:52.031273Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4c71afe89a849fcbe312889fc5db6ec"}},"metadata":{}}]},{"cell_type":"code","source":"model = \"/kaggle/working/final_model\"\n\n# Load the HF pipeline using our newly fine-tuned Gemma 2B\npipe_finetuned = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    model_kwargs={\"torch_dtype\": torch.float16},\n    device_map='auto',\n    max_new_tokens=512\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:11:52.033551Z","iopub.execute_input":"2024-03-04T00:11:52.033877Z","iopub.status.idle":"2024-03-04T00:11:54.831311Z","shell.execute_reply.started":"2024-03-04T00:11:52.033843Z","shell.execute_reply":"2024-03-04T00:11:54.830347Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbba8babe9724a6da9534376633ef3d6"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's see it in action on the same writeup we analyzed before:","metadata":{}},{"cell_type":"code","source":"outputs = pipe_finetuned(\n    prompt,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3,\n    add_special_tokens=True\n)\nprint(outputs[0][\"generated_text\"][len(prompt):])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:11:54.832576Z","iopub.execute_input":"2024-03-04T00:11:54.832875Z","iopub.status.idle":"2024-03-04T00:12:14.932901Z","shell.execute_reply.started":"2024-03-04T00:11:54.832849Z","shell.execute_reply":"2024-03-04T00:12:14.931957Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Sure, here's a summary of the text in a technical way:\n\n**Chapter 1: Data Preprocessing**\n\n* Extract 18 lip points, 20 pose points, and all hand points.\n* Apply various augmentations and normalization techniques.\n* Fill NaN values with zeros and use nearest interpolation for the time axis.\n\n**Chapter 2: Augmentation**\n\n* Implement common augmentations like random affine, replace augmentation, and time and frequency masking.\n* Use finger tree rotation for specific finger pairs.\n\n**Chapter 3: Training**\n\n* Train CNN and transformer models on one fold with a random split.\n* Use onecycle scheduler, weighted cross-entropy loss, and hypercolumn tuning.\n* Train EfficientNet-B0 and BERT on the full dataset.\n\n**Chapter 4: Submissions and Ensemble**\n\n* Aggregate models in a tf.Module class.\n* Calculate ensemble weights for fold 0 and apply to the full dataset.\n* Achieve leaderboard scores with EfficientNet-B0 and BERT ensemble.\n\n**Chapter 5: PS. Need Better TFlite DepthwiseConv2D**\n\n* Explore the differences between Conv2D and DepthwiseConv2D in TFlite.\n* Rewrite DepthwiseConv2D to achieve faster running time.\n* Ensemble two CNN models for better performance.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As we can see, our fine-tuned model is **still able to follow our instructions**. But how can we assess if the extra training worked? \n\nGiven that we **requested concise summaries** during the fine-tuning process, let's compare our base model with the fine-tuned model:","metadata":{}},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Write a short summary of 2-3 sentences of the following text:\\n\\n{}\".format(writeup)\n    }\n]\n\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(\n    prompt,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3,\n    add_special_tokens=True\n)\nprint(outputs[0][\"generated_text\"][len(prompt):])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:12:14.934129Z","iopub.execute_input":"2024-03-04T00:12:14.934441Z","iopub.status.idle":"2024-03-04T00:12:35.418961Z","shell.execute_reply.started":"2024-03-04T00:12:14.934404Z","shell.execute_reply":"2024-03-04T00:12:35.418011Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Sure, here's a summary of the text:\n\nThe text describes the training of an EfficientNet-B0 model and a transformer model on a dataset of lip and pose data. The models are trained using an ensemble approach, which combines models trained on different folds.\n\n**EfficientNet-B0:**\n\n* Uses a CNN architecture with 5 blocks.\n* Each block has a depthwise convolution operation followed by a residual connection.\n* The model is trained with a weighted cross-entropy loss function and an optimizer called Adam.\n\n**Transformer:**\n\n* Uses a transformer architecture with 4 layers and 256 hidden units.\n* The model is trained with a ranger optimizer and a loss function that encourages the model to predict the same labels as the training data.\n\n**Ensemble:**\n\n* The models are trained on different folds and then combined using an ensemble approach.\n* The weights for each model are learned from the corresponding fold.\n* The final ensemble includes EfficientNet-B0, BERT, and DeBERTa.\n\n**Key takeaways:**\n\n* The EfficientNet-B0 model achieved a leaderboard score of 0.8.\n* The transformer model achieved a leaderboard score of 0.81.\n* The ensemble approach significantly improved the performance of the models.\n* The EfficientNet-B0 model was faster than the full Conv2D model.\n","output_type":"stream"}]},{"cell_type":"code","source":"outputs = pipe_finetuned(\n    prompt,\n    do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3,\n    add_special_tokens=True\n)\nprint(outputs[0][\"generated_text\"][len(prompt):])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T00:12:35.420150Z","iopub.execute_input":"2024-03-04T00:12:35.420446Z","iopub.status.idle":"2024-03-04T00:12:45.824961Z","shell.execute_reply.started":"2024-03-04T00:12:35.420419Z","shell.execute_reply":"2024-03-04T00:12:45.824020Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Sure, here's a summary of the text:\n\nThe text describes the training of an EfficientNet-B0 model and a BERT model on a dataset of lip and pose data. The EfficientNet-B0 model achieved a leaderboard score of approximately 0.8, while the BERT model achieved a score of 0.81. The ensemble of these models achieved a leaderboard score of approximately 0.82.\n\nThe text also provides some insights into the training process, including the use of augmentation, the optimization of hyperparameters, and the conversion of DepthwiseConv2D operations to tflite.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It is clear that **fine-tuning improved significantly our output**: the model now **adheres to our instructions** and summarize in **few sentences the writeup**, whereas in its **previous** state the output was **lengthy.**\n\n> Looking ahead, it would be beneficial for the Kaggle community to develop a **domain-specific dataset** for fine-tuning Gemma on Kaggle write-ups. The curated summaries could adhere to a defined format, such as highlighting key aspects and identifying any omissions in the [Kaggle template](https://www.kaggle.com/solution-write-up-documentation). <br>\n\n\n<div class=\"alert alert-block alert-info\">\n<b>Key learnings </b><br><br>\n    - <b>Fine-tuning</b> proves beneficial in cases where <b>optimizing the prompt does not yield satisfactory results</b>. In these instances, a <b>domain-specific dataset</b> can greatly improve the desidered output.<br>\n    - Default (full weight) LLMs are <b>memory and compute-intensive</b>, which may render fine-tuning impractical. Parameter Efficient Fine-Tuning or <b>PEFT</b> is a potent approach that enables the model to be \"downsized,\" achieving <b>performance comparable to full fine-tuning with only a limited number of trainable parameters.</b> <br>\n</div>\n\n---\n\n# Conclusions and next steps\n\nIn this notebook, we explore the intricate world of text summarization using **Gemma capabilities** together with **HuggingFace and LangChain abstractions.** <br>\nWe progressed from merely requesting a summary to understard the **essential setup and parameters** required for meaningful results. This includes prompt engineering, chat template, chunking and summarization strategies and fine-tuning. <br>\n\n>Why is this useful? <br>\nFrom a Kaggle perspective, I foresee the possibility of having a personalized assistant on the website, helping users navigate through Kaggle complexity both on forum and on competition cards.\n\nThere are still one aspect I intend to explore in the coming days, that is to implement the findings outlined in the [QAGS paper](https://aclanthology.org/2020.acl-main.450.pdf). This poses an intriguing question: **how can we ensure we obtained factual consistency in our summary?**\n\nThe paper talks about a framework called **QAGS (Question Answering and Generation for Summarization)** and it can be summarized with one image:\n\n<img src=\"https://i.imgur.com/NX6P37e.png\" width=\"600\">\n\nThe idea I'll try to implement is the following:\n- Use a model to generate Q&A on the summary produced by Gemma\n- Collect answers for both the summary and the input text by querying a model\n- Examine consistency and identify hallucinations, that is if facts are present only in the summary but not in the original text.\n\n**Stay tuned, any feedback will be appreciated!**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}